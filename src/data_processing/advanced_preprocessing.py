"""
–ú–æ–¥—É–ª—å: src/data_processing/advanced_preprocessing.py

–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∞–Ω–∞–ª–∏–∑–æ–º –≤—ã–±—Ä–æ—Å–æ–≤, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–µ–π, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
    –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.

–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
    - CSV —Ñ–∞–π–ª data/processed/diabetic_data_top10.csv —Å —Ç–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏

–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
    - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ –∫–æ–Ω—Å–æ–ª—å (–ø—Ä–æ–ø—É—Å–∫–∏, –≤—ã–±—Ä–æ—Å—ã, –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)
    - –ì—Ä–∞—Ñ–∏–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ–∫–Ω–∞—Ö (—è—â–∏–∫–∏ —Å —É—Å–∞–º–∏, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏)
    - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è CatBoost (–º–µ—Ç—Ä–∏–∫–∏, –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
    - –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ)

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    - –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤—Ä—É—á–Ω—É—é: python -m src.data_processing.advanced_preprocessing
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —ç—Ç–∞–ø–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    - –ì—Ä–∞—Ñ–∏–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ–∫–Ω–∞—Ö (–≤ PyCharm - –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏)
    - –ú–æ–¥–µ–ª—å CatBoost –æ–±—É—á–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å –≤—ã–≤–æ–¥–æ–º –º–µ—Ç—Ä–∏–∫
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.preprocessing import label_binarize
import warnings
warnings.filterwarnings('ignore')

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç CatBoost
try:
    from catboost import CatBoostClassifier
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    print("‚ö†Ô∏è CatBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–æ.")
    print("   –î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏: pip install catboost")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ PyCharm
# –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ backends –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
try:
    import matplotlib
    # –î–ª—è PyCharm –∏—Å–ø–æ–ª—å–∑—É–µ–º Agg backend, –Ω–æ —Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ–∫–Ω–∞—Ö
    matplotlib.use('TkAgg', force=True)  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ TkAgg
    import matplotlib.pyplot as plt
    plt.switch_backend('TkAgg')  # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ TkAgg
except Exception as e:
    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å TkAgg backend: {e}")
    try:
        matplotlib.use('Qt5Agg', force=True)
        import matplotlib.pyplot as plt
        plt.switch_backend('Qt5Agg')
    except:
        try:
            matplotlib.use('Agg', force=True)  # –ë–µ–∑ GUI –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            print("‚ö†Ô∏è –ì—Ä–∞—Ñ–∏–∫–∏ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª—ã, –Ω–æ –Ω–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω—ã")
        except:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å matplotlib backend")

plt.ion()  # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
sns.set_style("whitegrid")

PROJECT_ROOT = Path(__file__).resolve().parents[2]
TOP10_PATH = PROJECT_ROOT / "data" / "processed" / "diabetic_data_top10.csv"

# –¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
NUMERICAL_FEATURES = [
    "number_inpatient",
    "number_diagnoses",
    "number_emergency",
    "number_outpatient",
    "time_in_hospital"
]

CATEGORICAL_FEATURES = [
    "diag_1",
    "diag_2",
    "diag_3",
    "medical_specialty",
    "diabetesMed"
]

TARGET = "readmitted"


def load_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å —Ç–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏"""
    if not TOP10_PATH.exists():
        raise FileNotFoundError(f"–§–∞–π–ª {TOP10_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ select_top_features.py")
    
    df = pd.read_csv(TOP10_PATH)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤")
    return df


def analyze_missing_values(df):
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö"""
    print("\n" + "="*60)
    print("–ê–ù–ê–õ–ò–ó –ü–†–û–ü–£–°–ö–û–í –í –î–ê–ù–ù–´–•")
    print("="*60)
    
    missing = df.isna().sum()
    missing_pct = (missing / len(df)) * 100
    
    missing_df = pd.DataFrame({
        '–°—Ç–æ–ª–±–µ—Ü': missing.index,
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤': missing.values,
        '–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ (%)': missing_pct.values
    })
    missing_df = missing_df[missing_df['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤'] > 0]
    
    if missing_df.empty:
        print("‚úÖ –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
    else:
        print(missing_df.to_string(index=False))
    
    return missing_df


def detect_outliers_iqr(df, feature):
    """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –º–µ—Ç–æ–¥–æ–º IQR (–º–µ–∂–∫–≤–∞—Ä—Ç–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–∞—Ö)"""
    Q1 = df[feature].quantile(0.25)
    Q3 = df[feature].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]
    
    return {
        'Q1': Q1,
        'Q3': Q3,
        'IQR': IQR,
        'lower_bound': lower_bound,
        'upper_bound': upper_bound,
        'outliers_count': len(outliers),
        'outliers_pct': (len(outliers) / len(df)) * 100
    }


def analyze_outliers(df):
    """–ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    print("\n" + "="*60)
    print("–ê–ù–ê–õ–ò–ó –í–´–ë–†–û–°–û–í (–ú–ï–¢–û–î IQR)")
    print("="*60)
    
    outliers_info = {}
    
    for feature in NUMERICAL_FEATURES:
        if feature in df.columns:
            info = detect_outliers_iqr(df, feature)
            outliers_info[feature] = info
            
            print(f"\nüìä {feature}:")
            print(f"   Q1 (25%): {info['Q1']:.2f}")
            print(f"   Q3 (75%): {info['Q3']:.2f}")
            print(f"   IQR: {info['IQR']:.2f}")
            print(f"   –ì—Ä–∞–Ω–∏—Ü—ã: [{info['lower_bound']:.2f}, {info['upper_bound']:.2f}]")
            print(f"   –í—ã–±—Ä–æ—Å–æ–≤: {info['outliers_count']} ({info['outliers_pct']:.2f}%)")
    
    return outliers_info


def plot_boxplots(df):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —è—â–∏–∫–æ–≤ —Å —É—Å–∞–º–∏ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    print("\n" + "="*60)
    print("–ü–û–°–¢–†–û–ï–ù–ò–ï –Ø–©–ò–ö–û–í –° –£–°–ê–ú–ò")
    print("="*60)
    
    num_features = [f for f in NUMERICAL_FEATURES if f in df.columns]
    n_features = len(num_features)
    
    if n_features == 0:
        print("‚ö†Ô∏è –ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤")
        return
    
    # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç–∫—É –≥—Ä–∞—Ñ–∏–∫–æ–≤
    cols = 2
    rows = (n_features + 1) // 2
    
    fig, axes = plt.subplots(rows, cols, figsize=(14, 4 * rows))
    fig.suptitle('–Ø—â–∏–∫–∏ —Å —É—Å–∞–º–∏ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–≤—ã—è–≤–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤)',
                 fontsize=16, fontweight='bold')
    
    if n_features == 1:
        axes = [axes]
    else:
        axes = axes.flatten()
    
    for idx, feature in enumerate(num_features):
        ax = axes[idx]
        df.boxplot(column=feature, ax=ax, vert=True)
        ax.set_title(f'{feature}', fontweight='bold')
        ax.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
        ax.grid(True, alpha=0.3)
    
    # –°–∫—Ä—ã–≤–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∏
    for idx in range(n_features, len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    plt.subplots_adjust(hspace=0.4, wspace=0.3, top=0.85)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –≥—Ä–∞—Ñ–∏–∫–∞–º–∏ –∏ –æ–ø—É—Å–∫–∞–µ–º –∏—Ö –Ω–∏–∂–µ
    plt.draw()
    plt.pause(0.1)
    print("‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —è—â–∏–∫–æ–≤ —Å —É—Å–∞–º–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã")


def plot_distributions(df):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    print("\n" + "="*60)
    print("–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –ß–ò–°–õ–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í")
    print("="*60)
    
    num_features = [f for f in NUMERICAL_FEATURES if f in df.columns]
    n_features = len(num_features)
    
    if n_features == 0:
        return
    
    cols = 2
    rows = (n_features + 1) // 2

    fig, axes = plt.subplots(rows, cols, figsize=(14, 5 * rows))
    fig.suptitle('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', fontsize=16, fontweight='bold')
    
    if n_features == 1:
        axes = [axes]
    else:
        axes = axes.flatten()
    
    for idx, feature in enumerate(num_features):
        ax = axes[idx]
        df[feature].hist(bins=30, ax=ax, edgecolor='black', alpha=0.7, color='steelblue')
        ax.set_title(f'{feature}', fontweight='bold')
        ax.set_xlabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
        ax.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
        ax.grid(True, alpha=0.3)
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–µ –ª–∏–Ω–∏–∏ –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ –∏ –º–µ–¥–∏–∞–Ω—ã
        ax.axvline(df[feature].mean(), color='red', linestyle='--', linewidth=2, label=f'–°—Ä–µ–¥–Ω–µ–µ: {df[feature].mean():.2f}')
        ax.axvline(df[feature].median(), color='green', linestyle='--', linewidth=2, label=f'–ú–µ–¥–∏–∞–Ω–∞: {df[feature].median():.2f}')
        ax.legend()
    
    for idx in range(n_features, len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    plt.subplots_adjust(hspace=0.4, wspace=0.3, top=0.85)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –≥—Ä–∞—Ñ–∏–∫–∞–º–∏ –∏ –æ–ø—É—Å–∫–∞–µ–º –∏—Ö –Ω–∏–∂–µ
    plt.draw()
    plt.pause(0.1)
    print("‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã")


def plot_correlation_heatmap(df):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    print("\n" + "="*60)
    print("–¢–ï–ü–õ–û–í–ê–Ø –ö–ê–†–¢–ê –ö–û–†–†–ï–õ–Ø–¶–ò–ô")
    print("="*60)
    
    num_features = [f for f in NUMERICAL_FEATURES if f in df.columns]
    
    if len(num_features) < 2:
        print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π")
        return
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é
    corr_matrix = df[num_features].corr()
    
    # –°—Ç—Ä–æ–∏–º —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É
    plt.figure(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0,
                square=True, linewidths=1, cbar_kws={"shrink": 0.8})
    plt.title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', fontsize=14, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.draw()
    plt.pause(0.1)
    print("‚úÖ –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞")
    
    # –í—ã–≤–æ–¥–∏–º —Ç–æ–ø –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    print("\n–¢–æ–ø-5 –ø–∞—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π:")
    corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_pairs.append((
                corr_matrix.columns[i],
                corr_matrix.columns[j],
                corr_matrix.iloc[i, j]
            ))
    corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)
    for feat1, feat2, corr in corr_pairs[:5]:
        print(f"   {feat1} ‚Üî {feat2}: {corr:.3f}")


def plot_target_distribution(df):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞"""
    if TARGET not in df.columns:
        return
    
    print("\n" + "="*60)
    print("–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¶–ï–õ–ï–í–û–ì–û –ü–†–ò–ó–ù–ê–ö–ê")
    print("="*60)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # –°—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
    value_counts = df[TARGET].value_counts()
    colors = ['#2ecc71', '#f39c12', '#e74c3c']  # –ó–µ–ª–µ–Ω—ã–π, –æ—Ä–∞–Ω–∂–µ–≤—ã–π, –∫—Ä–∞—Å–Ω—ã–π
    ax1.bar(value_counts.index, value_counts.values, color=colors[:len(value_counts)], 
           edgecolor='black', alpha=0.7)
    ax1.set_xlabel('–ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –≥–æ—Å–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏', fontweight='bold')
    ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', fontweight='bold')
    ax1.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º', fontweight='bold')
    ax1.grid(axis='y', alpha=0.3)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for i, (idx, val) in enumerate(value_counts.items()):
        ax1.text(idx, val, str(val), ha='center', va='bottom', fontweight='bold')
    
    # –ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
    labels = []
    for idx in value_counts.index:
        if idx == 'NO':
            labels.append('–ù–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–π\n–≥–æ—Å–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏')
        elif idx == '<30':
            labels.append('–ì–æ—Å–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è\n<30 –¥–Ω–µ–π')
        elif idx == '>30':
            labels.append('–ì–æ—Å–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è\n>30 –¥–Ω–µ–π')
        else:
            labels.append(str(idx))
    
    ax2.pie(value_counts.values, labels=labels, autopct='%1.1f%%', 
           colors=colors[:len(value_counts)], startangle=90, 
           explode=[0.05] * len(value_counts), shadow=True)
    ax2.set_title('–ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', fontweight='bold')
    
    plt.tight_layout()
    plt.draw()
    plt.pause(0.1)
    print("‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã")


def analyze_categorical_features(df):
    """–ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    print("\n" + "="*60)
    print("–ê–ù–ê–õ–ò–ó –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í")
    print("="*60)
    
    for feature in CATEGORICAL_FEATURES:
        if feature in df.columns:
            unique_count = df[feature].nunique()
            value_counts = df[feature].value_counts()
            
            print(f"\nüìã {feature}:")
            print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {unique_count}")
            print(f"   –¢–æ–ø-5 –∑–Ω–∞—á–µ–Ω–∏–π:")
            for val, count in value_counts.head(5).items():
                pct = (count / len(df)) * 100
                print(f"      {val}: {count} ({pct:.2f}%)")


def standardize_data(df):
    """–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Z-score –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)"""
    print("\n" + "="*60)
    print("–°–¢–ê–ù–î–ê–†–¢–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–• (Z-score)")
    print("="*60)
    
    df_standardized = df.copy()
    scaler = StandardScaler()
    
    num_features = [f for f in NUMERICAL_FEATURES if f in df.columns]
    
    if num_features:
        df_standardized[num_features] = scaler.fit_transform(df[num_features])
        
        print("‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
        print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –î–û —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏:")
        print(df[num_features].describe().round(2))
        
        print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ü–û–°–õ–ï —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏:")
        print(df_standardized[num_features].describe().round(2))
        
        print("\n–ü—Ä–æ–≤–µ—Ä–∫–∞: —Å—Ä–µ–¥–Ω–µ–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å ~0, std –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å ~1")
        print(f"–°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è: {df_standardized[num_features].mean().round(4).to_dict()}")
        print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è: {df_standardized[num_features].std().round(4).to_dict()}")
    else:
        print("‚ö†Ô∏è –ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏")
    
    return df_standardized, scaler


def normalize_data(df):
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (Min-Max scaling)"""
    print("\n" + "="*60)
    print("–ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–• (Min-Max scaling)")
    print("="*60)
    
    df_normalized = df.copy()
    scaler = MinMaxScaler()
    
    num_features = [f for f in NUMERICAL_FEATURES if f in df.columns]
    
    if num_features:
        df_normalized[num_features] = scaler.fit_transform(df[num_features])
        
        print("‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
        print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –î–û –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:")
        print(df[num_features].describe().round(2))
        
        print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ü–û–°–õ–ï –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:")
        print(df_normalized[num_features].describe().round(2))
        
        print("\n–ü—Ä–æ–≤–µ—Ä–∫–∞: –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1]")
        print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {df_normalized[num_features].min().round(4).to_dict()}")
        print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {df_normalized[num_features].max().round(4).to_dict()}")
    else:
        print("‚ö†Ô∏è –ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏")
    
    return df_normalized, scaler


def train_and_evaluate_catboost(df):
    """–û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ CatBoost"""
    print("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")

    target = TARGET
    if target not in df.columns:
        print(f"‚ùå –¶–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ '{target}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö")
        return None

    X = df.drop(columns=[target])
    y = df[target].astype(str)

    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è CatBoost
    cat_features = [col for col in X.columns if X[col].dtype == 'object']

    print(f"‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape[1]}, –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö: {len(cat_features)}")
    print(f"‚úÖ –†–∞–∑–º–µ—Ä —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞: {y.shape}")
    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {y.value_counts().to_dict()}")

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ CatBoost...")

    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = CatBoostClassifier(
        iterations=300,
        depth=6,
        learning_rate=0.1,
        loss_function="MultiClass",
        verbose=50,  # –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 50 –∏—Ç–µ—Ä–∞—Ü–∏–π
        random_seed=42
    )

    model.fit(X_train, y_train, cat_features=cat_features)

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)

    # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è ROC-AUC
    y_test_bin = label_binarize(y_test, classes=list(sorted(set(y_test))))

    try:
        roc_auc = roc_auc_score(y_test_bin, y_prob, average="macro", multi_class="ovr")
    except Exception as e:
        roc_auc = None
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å ROC-AUC: {e}")

    # –ú–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)

    print("\n" + "="*50)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø CATBOOST")
    print("="*50)
    print(f"Accuracy:  {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1-score:  {f1:.4f}")
    if roc_auc is not None:
        print(f"ROC-AUC:   {roc_auc:.4f}")

    print("\nConfusion Matrix:")
    print(cm)

    print("\n–ö–ª–∞—Å—Å—ã –º–æ–¥–µ–ª–∏:")
    for i, class_name in enumerate(sorted(set(y_test))):
        print(f"  {class_name}: –∏–Ω–¥–µ–∫—Å {i}")

    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = model.get_feature_importance()
    feature_names = X.columns

    print(f"\n–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)

    for idx, row in importance_df.head(10).iterrows():
        print(f"  {row['feature']}: {row['importance']:.4f}")

    return {
        'model': model,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'roc_auc': roc_auc,
        'confusion_matrix': cm,
        'feature_importance': importance_df
    }


def print_summary_statistics(df):
    """–í—ã–≤–æ–¥ –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –¥–∞–Ω–Ω—ã–º"""
    print("\n" + "="*60)
    print("–û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –î–ê–ù–ù–´–ú")
    print("="*60)
    
    print(f"\n–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape[0]} —Å—Ç—Ä–æ–∫ √ó {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
    
    print("\nüìä –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
    num_features = [f for f in NUMERICAL_FEATURES if f in df.columns]
    if num_features:
        print(df[num_features].describe().round(2))
    
    print("\nüìã –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
    cat_features = [f for f in CATEGORICAL_FEATURES if f in df.columns]
    for feature in cat_features:
        print(f"\n{feature}:")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {df[feature].nunique()}")
        print(f"   –¢–æ–ø-3: {df[feature].value_counts().head(3).to_dict()}")
    
    if TARGET in df.columns:
        print(f"\nüéØ –¶–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ ({TARGET}):")
        print(df[TARGET].value_counts())
        print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (%):")
        print((df[TARGET].value_counts(normalize=True) * 100).round(2))


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    print("="*60)
    print("–†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•")
    print("="*60)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = load_data()
    
    # 2. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤
    analyze_missing_values(df)
    
    # 3. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print_summary_statistics(df)
    
    # 4. –ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤
    outliers_info = analyze_outliers(df)
    
    # 5. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤—ã–±—Ä–æ—Å–æ–≤
    plot_boxplots(df)
    
    # 6. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
    plot_distributions(df)

    # 6.1. –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    plot_correlation_heatmap(df)

    # 6.2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
    plot_target_distribution(df)

    # 7. –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    analyze_categorical_features(df)

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –ø–∞—É–∑—É –º–µ–∂–¥—É –≥—Ä–∞—Ñ–∏–∫–∞–º–∏
    plt.pause(1)
    
    # 8. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
    df_standardized, scaler_std = standardize_data(df)

    # 9. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    df_normalized, scaler_minmax = normalize_data(df)

    print("\n" + "="*60)
    print("–ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ò")
    print("="*60)
    print(f"‚úÖ –ò—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(df.columns)}")
    print(f"‚úÖ –ß–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(NUMERICAL_FEATURES)}")
    print(f"‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(CATEGORICAL_FEATURES)}")
    print(f"‚úÖ –í—ã–±—Ä–æ—Å–æ–≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ: {sum(info['outliers_count'] for info in outliers_info.values())}")

    # 10. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ CatBoost (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
    if CATBOOST_AVAILABLE:
        print("\n" + "="*60)
        print("–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò CATBOOST")
        print("="*60)

        model_results = train_and_evaluate_catboost(df)
    else:
        print("\n" + "="*60)
        print("–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ü–†–û–ü–£–©–ï–ù–û")
        print("="*60)
        print("‚ö†Ô∏è CatBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏:")
        print("   pip install catboost")
        model_results = None

    print("\n" + "="*60)
    if CATBOOST_AVAILABLE:
        print("–ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –ò –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–´")
    else:
        print("–ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
    print("="*60)
    print("\nüí° –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ì—Ä–∞—Ñ–∏–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ–∫–Ω–∞—Ö.")
    print("   –ó–∞–∫—Ä–æ–π—Ç–µ –æ–∫–Ω–∞ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —Å–∫—Ä–∏–ø—Ç–∞.")

    return {
        'original': df,
        'standardized': df_standardized,
        'normalized': df_normalized,
        'outliers_info': outliers_info,
        'model_results': model_results
    }


if __name__ == "__main__":
    try:
        results = main()
        
        # –î–µ—Ä–∂–∏–º –≥—Ä–∞—Ñ–∏–∫–∏ –æ—Ç–∫—Ä—ã—Ç—ã–º–∏
        print("\n" + "="*60)
        print("–í—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã. –ó–∞–∫—Ä–æ–π—Ç–µ –æ–∫–Ω–∞ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.")
        print("\n" + "="*60)
        print("–í—Å–µ –≥–æ—Ç–æ–≤–æ! –ì—Ä–∞—Ñ–∏–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ–∫–Ω–∞—Ö.")
        print("–ó–∞–∫—Ä–æ–π—Ç–µ –≤—Å–µ –æ–∫–Ω–∞ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —Å–∫—Ä–∏–ø—Ç–∞.")
        print("="*60)

        # –î–µ—Ä–∂–∏–º —Å–∫—Ä–∏–ø—Ç –∞–∫—Ç–∏–≤–Ω—ã–º –¥–æ –∑–∞–∫—Ä—ã—Ç–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
        input("\n–ù–∞–∂–º–∏—Ç–µ Enter –ø–æ—Å–ª–µ –∑–∞–∫—Ä—ã—Ç–∏—è –≤—Å–µ—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤...")
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

